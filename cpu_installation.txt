# == create conda environment ==
- 1. running code in terminal: in .py file, click on the play button and then arrow turned vertically down and click "Run Python File in Dedicated Terminal" 
- 2. create conda environment: conda create --name grounding_dino_testing_conda_env
- 3. activate conda environment: conda activate grounding_dino_testing_conda_env
    how to check? 
        -> should show in terminal (grounding_dino_testing_conda_env) micahsan@MacBook-Air-231 fill_sam_testing_terminal %
        -> show all virtual environments: conda env list
- install python: brew install python (Python is installed as /opt/homebrew/bin/python3)
- (for cpu) install packages: conda install pytorch torchvision torchaudio -c pytorch

# == install grounding dino ==
grounding dino website: https://github.com/IDEA-Research/GroundingDINO
- 1. clone grounding dino from github
    git clone https://github.com/IDEA-Research/GroundingDINO.git 
- 2. change current directory to groundingdino folder
    cd GroundingDINO/
- 3. install the required dependecies in the current directory
    pip install -e .
- 4. download pre-trained model weights
    mkdir weights
    cd weights
    wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
    cd ..

== install SAM ==
- 1. clone sam repository from github
    git clone https://github.com/facebookresearch/segment-anything.git
    cd segment-anything
    pip install -e .
- 2. install dependencies essential for mask post-processing
    pip install opencv-python pycocotools matplotlib onnxruntime onnx
- 3. download sam weights
- 3. download a model checkpoint
    mkdir -p ~/weights
    wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P ~/weights


other downloads
!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights
CHECKPOINT_FILENAME = "sam_vit_h_4b8939.pth"
CHECKPOINT_PATH  = os.path.join(HOME, "weights", CHECKPOINT_FILENAME)
DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
MODEL_TYPE = "vit_h"

# sam useful imports
import os # for managing file paths and directories
import glob
import torch # for using SAM that uses PyTorch
import cv2 # tool for for image processing and  (ex: OpenCV)
import supervision as sv # helps to visualize SAM's output
import numpy as np # format SAM expects of numpy array
import matplotlib.pyplot as plt # for plotting images
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator # the neural network we will use for img segmentation

# installing new extensions from "fine-tune SAM video"
import random
from scipy import ndimage

=====
import torch
import cv2 # for image processing
import numpy as np
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

CHECKPOINT_FILENAME = "sam_vit_h_4b8939.pth"
CHECKPOINT_PATH = os.path.join(HOME, "weights", CHECKPOINT_FILENAME)
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MODEL_TYPE = "vit_h"
print("Successfully uploaded checkpoint path, device, and model type")

sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)
sam.to(device=DEVICE)

mask_generator = SamAutomaticMaskGenerator(sam)

image = cv2.imread("test_baseline_data/original_concept_all_lines_centered/potato_chip_Professional3.png")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
masks = mask_generator.generate(image)
